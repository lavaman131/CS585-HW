# ğŸ–– Hand Gesture Recognition Project

## ğŸ“š Problem Definition

The problem is to recognize sign-language hand gestures from a video stream. This is useful because it can be used to create human computer interfaces that are more accessible to people with hearing disabilities. My analysis assumes that the background is relatively static and that the hand is the only moving object in the video stream.

Some difficulties that I anticipate are:

- The hand can be in different orientations and positions in the video stream.
- The hand can be in different lighting conditions.
- The hand can be occluded by other objects in the video stream.
- The hand can be in motion.

The gestures are defined as follows:

- **One**: The thumb is extended and the other fingers are closed.
- **Two**: The thumb and the index finger are extended and the other fingers are closed.
- **Three**: The thumb, index finger, and middle finger are extended and the other fingers are closed.
- **Four**: The thumb, index finger, middle finger, and ring finger are extended and the little finger is closed.
- **Five**: All fingers are extended.

## ğŸ› ï¸ Method and Implementation

### Project Structure

The project is structured as follows:

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ a2
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ algorithms
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ classification.py
â”‚   â”‚   â””â”€â”€ segmentation.py
â”‚   â””â”€â”€ data
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ preprocessing.py
â”‚       â””â”€â”€ utils.py
â”œâ”€â”€ experiments
â”‚   â”œâ”€â”€ black_background
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ demo
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ white_background
â”‚       â”œâ”€â”€ ...
â”œâ”€â”€ main.py
â”œâ”€â”€ main.sh
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ templates
â”‚   â”œâ”€â”€ binary_images
â”‚   â”‚   â”œâ”€â”€ 1.png
â”‚   â”‚   â”œâ”€â”€ ...
â”‚   â”‚   â””â”€â”€ labels.csv
â”‚   â””â”€â”€ original
â”‚       â”œâ”€â”€ 1.png
â”‚       â”œâ”€â”€ ...
â”‚       â””â”€â”€ labels.csv
â””â”€â”€ tools
    â”œâ”€â”€ prepare_templates.py
    â””â”€â”€ prepare_templates.sh
```

I use binary image analysis followed by max contour detection for the segmentation of the hand. I also use template matching (with templates augmented via rotations to capture possible orientations of the hand) with the maximum normalized correlation coefficient for classifying the hand movement as the digit 1, 2, 3, 4, or 5.

## ğŸ”¬ Experiments

## ğŸ“ˆ Results

## ğŸ® Demo

### ğŸ“¦ Setup

The project is implemented in Python 3.10. The dependencies are managed using Poetry. To install the dependencies, run the following commands:

```bash
# create a virtual environment
python -m venv .venv
# activate the virtual environment
source .venv/bin/activate
# install the dependencies
poetry install
```

## ğŸš€ Usage

To run the demo, execute the following command:

```bash
./main.sh
```

## ğŸ—£ï¸ Discussion

## ğŸ† Conclusions

## ğŸ¬ Credits and Bibliography

[Gamma Correction](https://pyimagesearch.com/2015/10/05/opencv-gamma-correction/)

[Count Approximation](https://pyimagesearch.com/2021/10/06/opencv-contour-approximation/)